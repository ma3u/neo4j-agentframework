{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Discovery & Semantic Exploration\n",
    "\n",
    "This notebook helps you discover insights and patterns in your knowledge graph:\n",
    "- Semantic similarity exploration\n",
    "- Topic clustering and relationships\n",
    "- Content gaps and recommendations\n",
    "- Cross-document connections\n",
    "- Knowledge path finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from neo4j_rag import Neo4jRAG, RAGQueryEngine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "\n",
    "# Initialize connections\n",
    "rag = Neo4jRAG()\n",
    "engine = RAGQueryEngine(rag)\n",
    "print(\"✅ Connected to Neo4j RAG System\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Similarity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all chunks with embeddings\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        WHERE c.embedding IS NOT NULL\n",
    "        RETURN c.text as text, c.embedding as embedding, \n",
    "               d.source as source, d.category as category\n",
    "        LIMIT 100\n",
    "    \"\"\")\n",
    "    \n",
    "    chunks = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for record in result:\n",
    "        chunks.append({\n",
    "            'text': record['text'][:100],  # First 100 chars\n",
    "            'source': record['source'],\n",
    "            'category': record.get('category', 'unknown')\n",
    "        })\n",
    "        embeddings.append(record['embedding'])\n",
    "\n",
    "if embeddings:\n",
    "    # Calculate similarity matrix\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Find most similar pairs (excluding self-similarity)\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    \n",
    "    # Get top 10 most similar pairs\n",
    "    similar_pairs = []\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(i+1, len(chunks)):\n",
    "            similar_pairs.append((i, j, similarity_matrix[i, j]))\n",
    "    \n",
    "    similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"🔗 Most Similar Content Pairs:\\n\")\n",
    "    for idx, (i, j, score) in enumerate(similar_pairs[:5]):\n",
    "        print(f\"{idx+1}. Similarity: {score:.3f}\")\n",
    "        print(f\"   Text 1: {chunks[i]['text'][:50]}...\")\n",
    "        print(f\"   Text 2: {chunks[j]['text'][:50]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings:\n",
    "    # Perform K-means clustering\n",
    "    n_clusters = min(5, len(embeddings))  # Adjust based on data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "    \n",
    "    # Reduce dimensions for visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis', \n",
    "                         alpha=0.6, s=100)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('Knowledge Clusters in Embedding Space')\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers_2d = pca.transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers_2d[:, 0], centers_2d[:, 1], \n",
    "               marker='*', s=300, c='red', edgecolor='black', linewidth=2)\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze cluster composition\n",
    "    print(\"\\n📊 Cluster Analysis:\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_chunks = [chunks[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "        categories = [c['category'] for c in cluster_chunks]\n",
    "        category_counts = Counter(categories)\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_chunks)} chunks):\")\n",
    "        print(f\"  Categories: {dict(category_counts)}\")\n",
    "        if cluster_chunks:\n",
    "            print(f\"  Sample: {cluster_chunks[0]['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Gaps Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key concepts to check\n",
    "key_concepts = [\n",
    "    \"installation\",\n",
    "    \"configuration\", \n",
    "    \"performance tuning\",\n",
    "    \"security\",\n",
    "    \"backup\",\n",
    "    \"migration\",\n",
    "    \"troubleshooting\",\n",
    "    \"best practices\",\n",
    "    \"API reference\",\n",
    "    \"examples\"\n",
    "]\n",
    "\n",
    "coverage_results = []\n",
    "\n",
    "for concept in key_concepts:\n",
    "    # Search for each concept\n",
    "    results = rag.vector_search(concept, k=5)\n",
    "    \n",
    "    if results:\n",
    "        avg_score = np.mean([r['score'] for r in results])\n",
    "        max_score = max([r['score'] for r in results])\n",
    "        coverage_results.append({\n",
    "            'Concept': concept,\n",
    "            'Coverage': 'Good' if max_score > 0.7 else 'Partial' if max_score > 0.5 else 'Weak',\n",
    "            'Max Score': max_score,\n",
    "            'Avg Score': avg_score,\n",
    "            'Matches': len(results)\n",
    "        })\n",
    "    else:\n",
    "        coverage_results.append({\n",
    "            'Concept': concept,\n",
    "            'Coverage': 'Missing',\n",
    "            'Max Score': 0,\n",
    "            'Avg Score': 0,\n",
    "            'Matches': 0\n",
    "        })\n",
    "\n",
    "# Display coverage analysis\n",
    "df_coverage = pd.DataFrame(coverage_results)\n",
    "print(\"🔍 Knowledge Coverage Analysis:\\n\")\n",
    "print(df_coverage.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualize coverage\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Coverage levels\n",
    "coverage_counts = df_coverage['Coverage'].value_counts()\n",
    "colors = {'Good': 'green', 'Partial': 'yellow', 'Weak': 'orange', 'Missing': 'red'}\n",
    "ax1.bar(coverage_counts.index, coverage_counts.values, \n",
    "       color=[colors.get(x, 'gray') for x in coverage_counts.index])\n",
    "ax1.set_xlabel('Coverage Level')\n",
    "ax1.set_ylabel('Number of Concepts')\n",
    "ax1.set_title('Knowledge Coverage Distribution')\n",
    "\n",
    "# Score distribution\n",
    "ax2.barh(df_coverage['Concept'], df_coverage['Max Score'], color='steelblue')\n",
    "ax2.axvline(0.7, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax2.axvline(0.5, color='orange', linestyle='--', alpha=0.5, label='Weak threshold')\n",
    "ax2.set_xlabel('Maximum Similarity Score')\n",
    "ax2.set_title('Concept Coverage Scores')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "weak_concepts = df_coverage[df_coverage['Coverage'].isin(['Weak', 'Missing'])]['Concept'].tolist()\n",
    "if weak_concepts:\n",
    "    print(\"\\n⚠️ Recommended areas for content addition:\")\n",
    "    for concept in weak_concepts:\n",
    "        print(f\"  - {concept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Document Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find connections between documents based on shared topics\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d1:Document)-[:HAS_CHUNK]->(c1:Chunk)\n",
    "        MATCH (d2:Document)-[:HAS_CHUNK]->(c2:Chunk)\n",
    "        WHERE d1.id < d2.id\n",
    "        WITH d1, d2, \n",
    "             COLLECT(DISTINCT CASE WHEN c1.text CONTAINS 'Neo4j' AND c2.text CONTAINS 'Neo4j' THEN 'Neo4j' END) +\n",
    "             COLLECT(DISTINCT CASE WHEN c1.text CONTAINS 'graph' AND c2.text CONTAINS 'graph' THEN 'graph' END) +\n",
    "             COLLECT(DISTINCT CASE WHEN c1.text CONTAINS 'database' AND c2.text CONTAINS 'database' THEN 'database' END) +\n",
    "             COLLECT(DISTINCT CASE WHEN c1.text CONTAINS 'query' AND c2.text CONTAINS 'query' THEN 'query' END) as shared_topics\n",
    "        WHERE SIZE([t IN shared_topics WHERE t IS NOT NULL]) > 0\n",
    "        RETURN d1.source as doc1, d2.source as doc2, \n",
    "               [t IN shared_topics WHERE t IS NOT NULL] as topics,\n",
    "               SIZE([t IN shared_topics WHERE t IS NOT NULL]) as connection_strength\n",
    "        ORDER BY connection_strength DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    connections = []\n",
    "    for record in result:\n",
    "        connections.append({\n",
    "            'Document 1': record['doc1'][:30],\n",
    "            'Document 2': record['doc2'][:30],\n",
    "            'Shared Topics': ', '.join(record['topics']),\n",
    "            'Strength': record['connection_strength']\n",
    "        })\n",
    "\n",
    "if connections:\n",
    "    df_connections = pd.DataFrame(connections)\n",
    "    print(\"🔗 Cross-Document Connections:\\n\")\n",
    "    print(df_connections.to_string(index=False))\n",
    "    \n",
    "    # Create network visualization\n",
    "    if len(connections) > 0:\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for conn in connections:\n",
    "            G.add_edge(conn['Document 1'], conn['Document 2'], \n",
    "                      weight=conn['Strength'])\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "        \n",
    "        # Draw network\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                              node_size=1000, alpha=0.7)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "        \n",
    "        # Draw edges with varying thickness\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] for u, v in edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=[w*2 for w in weights], \n",
    "                              alpha=0.5, edge_color='gray')\n",
    "        \n",
    "        plt.title('Document Connection Network')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different search strategies\n",
    "test_query = \"How to optimize Neo4j performance?\"\n",
    "\n",
    "print(f\"🔍 Query: '{test_query}'\\n\")\n",
    "\n",
    "# 1. Vector Search\n",
    "print(\"1️⃣ Vector Search Results:\")\n",
    "vector_results = rag.vector_search(test_query, k=3)\n",
    "for i, result in enumerate(vector_results, 1):\n",
    "    print(f\"   {i}. Score: {result['score']:.3f} - {result['text'][:80]}...\")\n",
    "\n",
    "# 2. Hybrid Search\n",
    "print(\"\\n2️⃣ Hybrid Search Results:\")\n",
    "hybrid_results = rag.hybrid_search(test_query, k=3)\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"   {i}. Score: {result['score']:.3f} - {result['text'][:80]}...\")\n",
    "\n",
    "# 3. Query with Context\n",
    "print(\"\\n3️⃣ RAG Query with Answer Generation:\")\n",
    "response = engine.query(test_query)\n",
    "print(f\"   Answer: {response['answer'][:200]}...\")\n",
    "print(f\"   Sources used: {len(response['sources'])}\")\n",
    "\n",
    "# Compare result overlap\n",
    "vector_texts = set([r['text'][:50] for r in vector_results])\n",
    "hybrid_texts = set([r['text'][:50] for r in hybrid_results])\n",
    "overlap = vector_texts.intersection(hybrid_texts)\n",
    "\n",
    "print(f\"\\n📊 Search Strategy Comparison:\")\n",
    "print(f\"   Vector unique results: {len(vector_texts - hybrid_texts)}\")\n",
    "print(f\"   Hybrid unique results: {len(hybrid_texts - vector_texts)}\")\n",
    "print(f\"   Overlapping results: {len(overlap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Topic Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how topics are distributed across chunks\n",
    "topics_to_analyze = ['Neo4j', 'graph', 'database', 'vector', 'embedding', 'search']\n",
    "\n",
    "topic_distribution = {}\n",
    "\n",
    "with rag.driver.session() as session:\n",
    "    for topic in topics_to_analyze:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "            WHERE toLower(c.text) CONTAINS toLower($topic)\n",
    "            RETURN c.chunk_index as chunk_index, COUNT(*) as count\n",
    "            ORDER BY chunk_index\n",
    "        \"\"\", topic=topic)\n",
    "        \n",
    "        indices = []\n",
    "        counts = []\n",
    "        for record in result:\n",
    "            indices.append(record['chunk_index'])\n",
    "            counts.append(record['count'])\n",
    "        \n",
    "        topic_distribution[topic] = (indices, counts)\n",
    "\n",
    "# Visualize topic distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, topic in enumerate(topics_to_analyze):\n",
    "    if idx < len(axes):\n",
    "        indices, counts = topic_distribution[topic]\n",
    "        if indices:\n",
    "            axes[idx].bar(indices, counts, color='teal', alpha=0.7)\n",
    "            axes[idx].set_xlabel('Chunk Index')\n",
    "            axes[idx].set_ylabel('Occurrences')\n",
    "            axes[idx].set_title(f'Topic: \"{topic}\"')\n",
    "            axes[idx].grid(axis='y', alpha=0.3)\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "            axes[idx].set_title(f'Topic: \"{topic}\"')\n",
    "\n",
    "plt.suptitle('Topic Distribution Across Document Chunks')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n📊 Topic Coverage Summary:\")\n",
    "for topic in topics_to_analyze:\n",
    "    indices, counts = topic_distribution[topic]\n",
    "    if counts:\n",
    "        print(f\"  {topic}: {sum(counts)} occurrences across {len(set(indices))} unique chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Question Answering Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various question types\n",
    "questions = [\n",
    "    (\"Definition\", \"What is Neo4j?\"),\n",
    "    (\"How-to\", \"How do I create a graph database?\"),\n",
    "    (\"Comparison\", \"What's the difference between graph and relational databases?\"),\n",
    "    (\"Best Practice\", \"What are best practices for Neo4j?\"),\n",
    "    (\"Troubleshooting\", \"How to debug Neo4j queries?\")\n",
    "]\n",
    "\n",
    "qa_results = []\n",
    "\n",
    "for q_type, question in questions:\n",
    "    print(f\"\\n❓ [{q_type}] {question}\")\n",
    "    \n",
    "    # Get answer\n",
    "    response = engine.query(question, k=3)\n",
    "    \n",
    "    # Evaluate response\n",
    "    answer_length = len(response['answer'])\n",
    "    sources_count = len(response['sources'])\n",
    "    avg_relevance = np.mean(response['relevance_scores']) if response['relevance_scores'] else 0\n",
    "    \n",
    "    print(f\"💡 Answer: {response['answer'][:150]}...\")\n",
    "    print(f\"📊 Metrics: {sources_count} sources, {answer_length} chars, {avg_relevance:.3f} relevance\")\n",
    "    \n",
    "    qa_results.append({\n",
    "        'Type': q_type,\n",
    "        'Question': question[:40],\n",
    "        'Answer Length': answer_length,\n",
    "        'Sources': sources_count,\n",
    "        'Avg Relevance': avg_relevance\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "df_qa = pd.DataFrame(qa_results)\n",
    "print(\"\\n📊 Question Answering Performance:\")\n",
    "print(df_qa.to_string(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Content Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate content recommendations based on current knowledge\n",
    "def find_related_content(query_text, exclude_source=None, k=5):\n",
    "    \"\"\"Find related content to a given query\"\"\"\n",
    "    results = rag.vector_search(query_text, k=k*2)  # Get more to filter\n",
    "    \n",
    "    # Filter and deduplicate\n",
    "    recommendations = []\n",
    "    seen_texts = set()\n",
    "    \n",
    "    for result in results:\n",
    "        text_preview = result['text'][:100]\n",
    "        if text_preview not in seen_texts:\n",
    "            if not exclude_source or result.get('metadata', {}).get('source') != exclude_source:\n",
    "                recommendations.append(result)\n",
    "                seen_texts.add(text_preview)\n",
    "        \n",
    "        if len(recommendations) >= k:\n",
    "            break\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test with a sample document chunk\n",
    "sample_text = \"Neo4j uses Cypher query language for graph traversal\"\n",
    "\n",
    "print(f\"📝 Reference Content: '{sample_text}'\\n\")\n",
    "print(\"🎯 Recommended Related Content:\\n\")\n",
    "\n",
    "recommendations = find_related_content(sample_text, k=5)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Relevance: {rec['score']:.3f}\")\n",
    "    print(f\"   Content: {rec['text'][:100]}...\")\n",
    "    if 'metadata' in rec and 'source' in rec['metadata']:\n",
    "        print(f\"   Source: {rec['metadata']['source']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Knowledge Graph Insights Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights report\n",
    "insights = {\n",
    "    'discovery_date': pd.Timestamp.now().isoformat(),\n",
    "    'knowledge_coverage': {},\n",
    "    'topic_clusters': {},\n",
    "    'content_gaps': [],\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "# Knowledge coverage\n",
    "stats = rag.get_stats()\n",
    "insights['knowledge_coverage'] = {\n",
    "    'total_documents': stats['documents'],\n",
    "    'total_chunks': stats['chunks'],\n",
    "    'avg_chunks_per_doc': stats['chunks'] / max(stats['documents'], 1)\n",
    "}\n",
    "\n",
    "# Topic clusters (from earlier analysis)\n",
    "if 'n_clusters' in locals():\n",
    "    insights['topic_clusters'] = {\n",
    "        'number_of_clusters': n_clusters,\n",
    "        'clustering_method': 'KMeans'\n",
    "    }\n",
    "\n",
    "# Content gaps (from coverage analysis)\n",
    "if 'weak_concepts' in locals():\n",
    "    insights['content_gaps'] = weak_concepts\n",
    "\n",
    "# Recommendations\n",
    "insights['recommendations'] = [\n",
    "    \"Add more content on: \" + \", \".join(insights['content_gaps'][:3]) if insights['content_gaps'] else \"Knowledge base is well-covered\",\n",
    "    f\"Current focus areas: {len(set([c.get('category', 'unknown') for c in chunks[:20]]))} distinct categories\",\n",
    "    \"Consider adding more examples and troubleshooting guides\" if 'examples' in weak_concepts else \"Example coverage is good\"\n",
    "]\n",
    "\n",
    "# Display insights\n",
    "print(\"🎯 Knowledge Discovery Insights:\\n\")\n",
    "print(\"📊 Coverage:\")\n",
    "for key, value in insights['knowledge_coverage'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n🔍 Content Gaps:\")\n",
    "for gap in insights['content_gaps'][:5]:\n",
    "    print(f\"   - {gap}\")\n",
    "\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "for rec in insights['recommendations']:\n",
    "    print(f\"   • {rec}\")\n",
    "\n",
    "# Save insights\n",
    "import json\n",
    "with open('knowledge_discovery_insights.json', 'w') as f:\n",
    "    json.dump(insights, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n✅ Insights saved to knowledge_discovery_insights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "rag.close()\n",
    "print(\"✅ Connections closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided deep insights into your knowledge graph:\n",
    "\n",
    "- **Semantic Networks**: Discovered relationships between content\n",
    "- **Topic Clustering**: Identified natural content groupings\n",
    "- **Knowledge Gaps**: Found areas needing more documentation\n",
    "- **Cross-Document Connections**: Mapped relationships between documents\n",
    "- **Search Patterns**: Compared different search strategies\n",
    "- **Content Recommendations**: Generated related content suggestions\n",
    "\n",
    "### Key Takeaways:\n",
    "1. Your knowledge graph shows natural clustering around key topics\n",
    "2. Some content gaps exist that could be filled for better coverage\n",
    "3. Hybrid search provides better results than vector search alone\n",
    "4. Documents are well-connected through shared topics\n",
    "\n",
    "### Next Steps:\n",
    "- Review **05_query_optimization.ipynb** for performance tuning\n",
    "- Add content to fill identified knowledge gaps\n",
    "- Use insights to improve document organization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}