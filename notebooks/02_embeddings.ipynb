{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embeddings in RAG\n",
    "\n",
    "This notebook explores:\n",
    "1. What embeddings are and how they work\n",
    "2. Generating embeddings with Sentence Transformers\n",
    "3. Calculating similarity between texts\n",
    "4. Visualizing embedding spaces\n",
    "5. Impact of different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Are Embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example text\n",
    "text = \"Neo4j is a graph database\"\n",
    "\n",
    "# Generate embedding\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"\\nEmbedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"\\nFirst 10 values: {embedding[:10]}\")\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"  Min: {embedding.min():.4f}\")\n",
    "print(f\"  Max: {embedding.max():.4f}\")\n",
    "print(f\"  Mean: {embedding.mean():.4f}\")\n",
    "print(f\"  Std: {embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different texts\n",
    "texts = [\n",
    "    \"Neo4j is a graph database\",\n",
    "    \"Graph databases store data as nodes and relationships\",\n",
    "    \"Neo4j uses Cypher query language\",\n",
    "    \"MongoDB is a document database\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all texts\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Calculate similarity matrix (cosine similarity)\n",
    "similarity_matrix = np.dot(embeddings, embeddings.T) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).reshape(-1, 1))\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_sim = pd.DataFrame(similarity_matrix, \n",
    "                      index=[f\"Text {i+1}\" for i in range(len(texts))],\n",
    "                      columns=[f\"Text {i+1}\" for i in range(len(texts))])\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_sim, annot=True, fmt='.3f', cmap='coolwarm', center=0.5,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Semantic Similarity Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show text mapping\n",
    "print(\"Text Mapping:\")\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"Text {i}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding Similar Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query text\n",
    "query = \"How to query a graph database?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Calculate similarities to all texts\n",
    "similarities = np.dot(embeddings, query_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Most similar texts:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(sorted_indices, 1):\n",
    "    print(f\"{rank}. Similarity: {similarities[idx]:.4f}\")\n",
    "    print(f\"   Text: {texts[idx]}\")\n",
    "    print()\n",
    "\n",
    "# Visualize as bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(texts)), similarities[sorted_indices], color='steelblue')\n",
    "plt.yticks(range(len(texts)), [f\"{texts[i][:30]}...\" if len(texts[i]) > 30 else texts[i] for i in sorted_indices])\n",
    "plt.xlabel('Similarity Score')\n",
    "plt.title(f'Similarity to Query: \"{query}\"')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Embedding Space (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more diverse texts for visualization\n",
    "categories = {\n",
    "    \"Graph Databases\": [\n",
    "        \"Neo4j is a graph database\",\n",
    "        \"Graph databases use nodes and edges\",\n",
    "        \"Cypher is Neo4j's query language\",\n",
    "        \"Graph traversal is efficient in Neo4j\"\n",
    "    ],\n",
    "    \"Document Databases\": [\n",
    "        \"MongoDB stores JSON documents\",\n",
    "        \"Document databases are schema-flexible\",\n",
    "        \"CouchDB is another document database\",\n",
    "        \"Documents can contain nested data\"\n",
    "    ],\n",
    "    \"Machine Learning\": [\n",
    "        \"Neural networks learn patterns\",\n",
    "        \"Deep learning uses multiple layers\",\n",
    "        \"Embeddings represent text as vectors\",\n",
    "        \"Transformers revolutionized NLP\"\n",
    "    ],\n",
    "    \"General\": [\n",
    "        \"The weather is sunny today\",\n",
    "        \"Coffee is a popular beverage\",\n",
    "        \"Books contain knowledge\",\n",
    "        \"Music brings joy to people\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten texts and create labels\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "all_colors = []\n",
    "color_map = {'Graph Databases': 'blue', 'Document Databases': 'green', \n",
    "             'Machine Learning': 'red', 'General': 'gray'}\n",
    "\n",
    "for category, texts in categories.items():\n",
    "    all_texts.extend(texts)\n",
    "    all_labels.extend([category] * len(texts))\n",
    "    all_colors.extend([color_map[category]] * len(texts))\n",
    "\n",
    "# Generate embeddings\n",
    "all_embeddings = model.encode(all_texts)\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d_pca = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# Also try t-SNE for comparison\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "embeddings_2d_tsne = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA visualization\n",
    "for category in categories.keys():\n",
    "    mask = np.array(all_labels) == category\n",
    "    ax1.scatter(embeddings_2d_pca[mask, 0], embeddings_2d_pca[mask, 1], \n",
    "               label=category, color=color_map[category], s=100, alpha=0.7)\n",
    "\n",
    "ax1.set_title('Embedding Space - PCA Projection', fontsize=14)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# t-SNE visualization\n",
    "for category in categories.keys():\n",
    "    mask = np.array(all_labels) == category\n",
    "    ax2.scatter(embeddings_2d_tsne[mask, 0], embeddings_2d_tsne[mask, 1], \n",
    "               label=category, color=color_map[category], s=100, alpha=0.7)\n",
    "\n",
    "ax2.set_title('Embedding Space - t-SNE Projection', fontsize=14)\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how similar texts cluster together in the embedding space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Impact of Different Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different embedding models\n",
    "models_to_compare = [\n",
    "    ('all-MiniLM-L6-v2', 384),  # Our default model\n",
    "    ('all-MiniLM-L12-v2', 384),  # Deeper model\n",
    "    # ('all-mpnet-base-v2', 768),  # Larger model (optional, slower)\n",
    "]\n",
    "\n",
    "test_texts = [\n",
    "    \"Neo4j graph database\",\n",
    "    \"Graph database technology\",\n",
    "    \"MongoDB document store\",\n",
    "    \"Weather forecast today\"\n",
    "]\n",
    "\n",
    "print(\"Comparing embedding models:\\n\")\n",
    "\n",
    "for model_name, dim in models_to_compare:\n",
    "    print(f\"Model: {model_name} ({dim} dimensions)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(test_texts)\n",
    "    \n",
    "    # Calculate similarity to first text\n",
    "    query_embedding = embeddings[0]\n",
    "    similarities = np.dot(embeddings[1:], query_embedding) / (\n",
    "        np.linalg.norm(embeddings[1:], axis=1) * np.linalg.norm(query_embedding)\n",
    "    )\n",
    "    \n",
    "    print(f\"Query: '{test_texts[0]}'\")\n",
    "    for i, sim in enumerate(similarities):\n",
    "        print(f\"  â†’ '{test_texts[i+1]}': {sim:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedding Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test embedding speed\n",
    "test_corpus = [\n",
    "    \"This is a test sentence.\" for _ in range(100)\n",
    "]\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single encoding\n",
    "start = time.time()\n",
    "for text in test_corpus:\n",
    "    _ = model.encode(text)\n",
    "single_time = time.time() - start\n",
    "\n",
    "# Batch encoding\n",
    "start = time.time()\n",
    "_ = model.encode(test_corpus)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"Encoding {len(test_corpus)} sentences:\\n\")\n",
    "print(f\"Single encoding (one by one): {single_time:.3f} seconds\")\n",
    "print(f\"Batch encoding (all at once): {batch_time:.3f} seconds\")\n",
    "print(f\"\\nSpeedup: {single_time/batch_time:.1f}x faster with batch encoding\")\n",
    "print(f\"\\nAverage time per sentence:\")\n",
    "print(f\"  Single: {single_time/len(test_corpus)*1000:.2f} ms\")\n",
    "print(f\"  Batch: {batch_time/len(test_corpus)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_rag import Neo4jRAG\n",
    "\n",
    "# Connect to Neo4j\n",
    "rag = Neo4jRAG()\n",
    "\n",
    "# Sample knowledge base about databases\n",
    "knowledge_base = [\n",
    "    \"Neo4j is a graph database that uses nodes and relationships to store data.\",\n",
    "    \"Graph databases are optimized for traversing connected data.\",\n",
    "    \"Cypher is Neo4j's declarative query language for pattern matching.\",\n",
    "    \"ACID compliance ensures data consistency in Neo4j transactions.\",\n",
    "    \"Neo4j can scale horizontally with causal clustering.\",\n",
    "    \"Vector indexes in Neo4j enable similarity search on embeddings.\",\n",
    "    \"MongoDB is a document database that stores data in JSON-like format.\",\n",
    "    \"PostgreSQL is a relational database with SQL support.\"\n",
    "]\n",
    "\n",
    "# Load knowledge base\n",
    "for i, text in enumerate(knowledge_base):\n",
    "    rag.add_document(\n",
    "        content=text,\n",
    "        metadata={\"source\": \"knowledge_base\", \"index\": i}\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Loaded {len(knowledge_base)} documents\\n\")\n",
    "\n",
    "# Test different queries\n",
    "queries = [\n",
    "    \"How does Neo4j ensure data consistency?\",\n",
    "    \"What query language does Neo4j use?\",\n",
    "    \"Can Neo4j do similarity search?\",\n",
    "    \"What is MongoDB?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = rag.vector_search(query, k=2)\n",
    "    \n",
    "    if results:\n",
    "        print(\"Top results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. (Score: {result['score']:.3f}) {result['text']}\")\n",
    "    print()\n",
    "\n",
    "# Clean up\n",
    "rag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Embeddings** are numerical representations of text (384-dimensional vectors in our case)\n",
    "2. **Semantic similarity** can be measured using cosine similarity between embeddings\n",
    "3. **Similar texts cluster together** in the embedding space\n",
    "4. **Different models** produce different embeddings with varying quality and speed\n",
    "5. **Batch encoding** is much faster than encoding texts one by one\n",
    "6. **RAG systems** use embeddings to find relevant context for answering questions\n",
    "\n",
    "### Key Takeaways:\n",
    "- Embeddings capture semantic meaning, not just keywords\n",
    "- The choice of embedding model affects both quality and performance\n",
    "- Visualization helps understand how embeddings organize information\n",
    "- In production, always use batch encoding for better performance\n",
    "\n",
    "### Next Steps:\n",
    "- Explore notebook 03 for document processing techniques\n",
    "- Try different embedding models for your use case\n",
    "- Experiment with different similarity thresholds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}