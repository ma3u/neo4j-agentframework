{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis & Visualization\n",
    "\n",
    "This notebook explores the knowledge graph structure in Neo4j:\n",
    "- Graph statistics and metrics\n",
    "- Node and relationship analysis\n",
    "- Visualization of document-chunk relationships\n",
    "- Community detection and clustering\n",
    "- Graph patterns and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from neo4j_rag import Neo4jRAG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Connect to Neo4j\n",
    "rag = Neo4jRAG()\n",
    "print(\"✅ Connected to Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics\n",
    "with rag.driver.session() as session:\n",
    "    # Overall counts\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        RETURN \n",
    "            COUNT(DISTINCT d) as total_documents,\n",
    "            COUNT(c) as total_chunks,\n",
    "            AVG(SIZE(d.content)) as avg_doc_size,\n",
    "            MIN(SIZE(d.content)) as min_doc_size,\n",
    "            MAX(SIZE(d.content)) as max_doc_size\n",
    "    \"\"\")\n",
    "    \n",
    "    stats = result.single()\n",
    "    \n",
    "print(\"📊 Graph Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Documents: {stats['total_documents']}\")\n",
    "print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "print(f\"\\nDocument Size Statistics:\")\n",
    "print(f\"  Average: {stats['avg_doc_size']:.0f} characters\")\n",
    "print(f\"  Minimum: {stats['min_doc_size']} characters\")\n",
    "print(f\"  Maximum: {stats['max_doc_size']} characters\")\n",
    "print(f\"\\nAverage chunks per document: {stats['total_chunks']/stats['total_documents']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Categories and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document categories\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        RETURN \n",
    "            d.category as category,\n",
    "            COUNT(DISTINCT d) as doc_count,\n",
    "            COUNT(c) as chunk_count,\n",
    "            AVG(SIZE(d.content)) as avg_size\n",
    "        ORDER BY doc_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    categories = []\n",
    "    for record in result:\n",
    "        categories.append({\n",
    "            'Category': record.get('category', 'Unknown'),\n",
    "            'Documents': record['doc_count'],\n",
    "            'Chunks': record['chunk_count'],\n",
    "            'Avg Size': f\"{record['avg_size']:.0f}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_categories = pd.DataFrame(categories)\n",
    "print(\"📁 Document Categories:\")\n",
    "print(df_categories.to_string(index=False))\n",
    "\n",
    "# Visualize distribution\n",
    "if len(categories) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Documents by category\n",
    "    ax1.bar(df_categories['Category'], df_categories['Documents'], color='steelblue')\n",
    "    ax1.set_xlabel('Category')\n",
    "    ax1.set_ylabel('Number of Documents')\n",
    "    ax1.set_title('Documents by Category')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Chunks by category\n",
    "    ax2.bar(df_categories['Category'], df_categories['Chunks'], color='coral')\n",
    "    ax2.set_xlabel('Category')\n",
    "    ax2.set_ylabel('Number of Chunks')\n",
    "    ax2.set_title('Chunks by Category')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunk Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk distribution across documents\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        RETURN \n",
    "            d.source as source,\n",
    "            COUNT(c) as chunk_count,\n",
    "            AVG(SIZE(c.text)) as avg_chunk_size,\n",
    "            MIN(c.chunk_index) as min_index,\n",
    "            MAX(c.chunk_index) as max_index\n",
    "        ORDER BY chunk_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    chunk_dist = []\n",
    "    for record in result:\n",
    "        chunk_dist.append({\n",
    "            'Source': record['source'][:30] + '...' if len(record['source']) > 30 else record['source'],\n",
    "            'Chunks': record['chunk_count'],\n",
    "            'Avg Chunk Size': f\"{record['avg_chunk_size']:.0f}\",\n",
    "            'Index Range': f\"{record['min_index']}-{record['max_index']}\"\n",
    "        })\n",
    "\n",
    "# Display top documents by chunk count\n",
    "df_chunks = pd.DataFrame(chunk_dist[:10])  # Top 10\n",
    "print(\"\\n📊 Top Documents by Chunk Count:\")\n",
    "print(df_chunks.to_string(index=False))\n",
    "\n",
    "# Visualize chunk distribution\n",
    "if len(chunk_dist) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    chunk_counts = [d['Chunks'] for d in chunk_dist]\n",
    "    plt.hist(chunk_counts, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Number of Chunks per Document')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Chunks Across Documents')\n",
    "    plt.axvline(np.mean(chunk_counts), color='red', linestyle='--', label=f'Mean: {np.mean(chunk_counts):.1f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Coverage and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedding coverage\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        RETURN \n",
    "            COUNT(c) as total_chunks,\n",
    "            COUNT(c.embedding) as chunks_with_embedding,\n",
    "            AVG(SIZE(c.embedding)) as avg_embedding_size\n",
    "    \"\"\")\n",
    "    \n",
    "    embed_stats = result.single()\n",
    "\n",
    "print(\"🧮 Embedding Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Chunks: {embed_stats['total_chunks']}\")\n",
    "print(f\"Chunks with Embeddings: {embed_stats['chunks_with_embedding']}\")\n",
    "print(f\"Coverage: {embed_stats['chunks_with_embedding']/embed_stats['total_chunks']*100:.1f}%\")\n",
    "print(f\"Embedding Dimensions: {embed_stats['avg_embedding_size']:.0f}\")\n",
    "\n",
    "# Sample embedding analysis\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        WHERE c.embedding IS NOT NULL\n",
    "        RETURN c.embedding[0..5] as sample_values\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    \n",
    "    sample = result.single()\n",
    "    if sample:\n",
    "        print(f\"\\nSample embedding values (first 5): {sample['sample_values']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Metadata Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document metadata\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        RETURN \n",
    "            d.source as source,\n",
    "            d.category as category,\n",
    "            d.topic as topic,\n",
    "            d.author as author,\n",
    "            d.created as created,\n",
    "            SIZE(d.content) as size\n",
    "        ORDER BY size DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    documents = []\n",
    "    for record in result:\n",
    "        documents.append({\n",
    "            'Source': record['source'][:25] + '...' if len(record.get('source', '')) > 25 else record.get('source', 'Unknown'),\n",
    "            'Category': record.get('category', 'N/A'),\n",
    "            'Topic': record.get('topic', 'N/A'),\n",
    "            'Author': record.get('author', 'N/A')[:15],\n",
    "            'Size': f\"{record['size']:,}\"\n",
    "        })\n",
    "\n",
    "# Display document metadata\n",
    "df_docs = pd.DataFrame(documents)\n",
    "print(\"\\n📄 Document Metadata:\")\n",
    "print(df_docs.to_string(index=False))\n",
    "\n",
    "# Metadata completeness\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        RETURN \n",
    "            COUNT(d) as total,\n",
    "            COUNT(d.category) as with_category,\n",
    "            COUNT(d.topic) as with_topic,\n",
    "            COUNT(d.author) as with_author,\n",
    "            COUNT(d.created) as with_created\n",
    "    \"\"\")\n",
    "    \n",
    "    meta_complete = result.single()\n",
    "\n",
    "print(\"\\n📊 Metadata Completeness:\")\n",
    "for field in ['category', 'topic', 'author', 'created']:\n",
    "    count = meta_complete[f'with_{field}']\n",
    "    total = meta_complete['total']\n",
    "    print(f\"  {field.capitalize()}: {count}/{total} ({count/total*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph connectivity\n",
    "with rag.driver.session() as session:\n",
    "    # Orphaned chunks (should be 0)\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        WHERE NOT (c)<-[:HAS_CHUNK]-()\n",
    "        RETURN COUNT(c) as orphaned_chunks\n",
    "    \"\"\")\n",
    "    orphaned = result.single()['orphaned_chunks']\n",
    "    \n",
    "    # Documents without chunks\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        WHERE NOT (d)-[:HAS_CHUNK]->()\n",
    "        RETURN COUNT(d) as docs_without_chunks\n",
    "    \"\"\")\n",
    "    no_chunks = result.single()['docs_without_chunks']\n",
    "    \n",
    "    # Relationship statistics\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH ()-[r:HAS_CHUNK]->()\n",
    "        RETURN COUNT(r) as total_relationships\n",
    "    \"\"\")\n",
    "    total_rels = result.single()['total_relationships']\n",
    "\n",
    "print(\"🔗 Graph Connectivity:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total HAS_CHUNK relationships: {total_rels}\")\n",
    "print(f\"Orphaned chunks: {orphaned}\")\n",
    "print(f\"Documents without chunks: {no_chunks}\")\n",
    "\n",
    "if orphaned > 0:\n",
    "    print(\"⚠️ Warning: Found orphaned chunks. Consider cleanup.\")\n",
    "if no_chunks > 0:\n",
    "    print(\"⚠️ Warning: Found documents without chunks. May need re-processing.\")\n",
    "else:\n",
    "    print(\"✅ Graph integrity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Search Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze similarity distribution for sample queries\n",
    "test_queries = [\n",
    "    \"What is Neo4j?\",\n",
    "    \"How does graph database work?\",\n",
    "    \"RAG implementation\",\n",
    "    \"Vector embeddings\"\n",
    "]\n",
    "\n",
    "similarity_data = []\n",
    "\n",
    "for query in test_queries:\n",
    "    results = rag.vector_search(query, k=10)\n",
    "    if results:\n",
    "        scores = [r['score'] for r in results]\n",
    "        similarity_data.append({\n",
    "            'Query': query,\n",
    "            'Max Score': max(scores),\n",
    "            'Min Score': min(scores),\n",
    "            'Avg Score': np.mean(scores),\n",
    "            'Std Dev': np.std(scores)\n",
    "        })\n",
    "\n",
    "if similarity_data:\n",
    "    df_sim = pd.DataFrame(similarity_data)\n",
    "    print(\"\\n🔍 Similarity Score Analysis:\")\n",
    "    print(df_sim.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Visualize similarity distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, query in enumerate(test_queries[:4]):\n",
    "        results = rag.vector_search(query, k=10)\n",
    "        if results:\n",
    "            scores = [r['score'] for r in results]\n",
    "            axes[idx].bar(range(len(scores)), scores, color='teal')\n",
    "            axes[idx].set_xlabel('Result Rank')\n",
    "            axes[idx].set_ylabel('Similarity Score')\n",
    "            axes[idx].set_title(f'Query: \"{query[:20]}...\"' if len(query) > 20 else f'Query: \"{query}\"')\n",
    "            axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Knowledge Coverage Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a knowledge coverage heatmap\n",
    "topics = ['Neo4j', 'Graph', 'Database', 'Vector', 'RAG', 'Embedding', 'Search', 'Query']\n",
    "categories_list = ['tutorial', 'guide', 'documentation', 'example']\n",
    "\n",
    "# Build coverage matrix\n",
    "coverage_matrix = np.zeros((len(topics), len(categories_list)))\n",
    "\n",
    "with rag.driver.session() as session:\n",
    "    for i, topic in enumerate(topics):\n",
    "        for j, category in enumerate(categories_list):\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "                WHERE toLower(c.text) CONTAINS toLower($topic)\n",
    "                  AND (d.category = $category OR $category = 'any')\n",
    "                RETURN COUNT(DISTINCT c) as count\n",
    "            \"\"\", topic=topic, category=category)\n",
    "            \n",
    "            coverage_matrix[i, j] = result.single()['count']\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(coverage_matrix, \n",
    "            xticklabels=categories_list,\n",
    "            yticklabels=topics,\n",
    "            annot=True,\n",
    "            fmt='.0f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Number of Chunks'})\n",
    "plt.title('Knowledge Coverage Heatmap\\n(Topics vs Categories)')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Topic')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n📊 Coverage Summary:\")\n",
    "print(f\"Most covered topic: {topics[np.argmax(coverage_matrix.sum(axis=1))]}\")\n",
    "print(f\"Most populated category: {categories_list[np.argmax(coverage_matrix.sum(axis=0))]}\")\n",
    "print(f\"Total topic mentions: {int(coverage_matrix.sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Graph Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export graph data for external analysis\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "export_data = {\n",
    "    'export_date': datetime.now().isoformat(),\n",
    "    'statistics': {},\n",
    "    'documents': [],\n",
    "    'sample_chunks': []\n",
    "}\n",
    "\n",
    "# Get statistics\n",
    "export_data['statistics'] = rag.get_stats()\n",
    "\n",
    "# Get document list\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)\n",
    "        OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        RETURN d.id as id, d.source as source, d.category as category,\n",
    "               COUNT(c) as chunk_count, AVG(SIZE(c.text)) as avg_chunk_size\n",
    "    \"\"\")\n",
    "    \n",
    "    for record in result:\n",
    "        export_data['documents'].append({\n",
    "            'id': record['id'],\n",
    "            'source': record['source'],\n",
    "            'category': record.get('category'),\n",
    "            'chunk_count': record['chunk_count'],\n",
    "            'avg_chunk_size': record['avg_chunk_size']\n",
    "        })\n",
    "\n",
    "# Get sample chunks\n",
    "with rag.driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        RETURN c.text as text, c.chunk_index as index, d.source as source\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    for record in result:\n",
    "        export_data['sample_chunks'].append({\n",
    "            'text': record['text'][:100] + '...',\n",
    "            'index': record['index'],\n",
    "            'source': record['source']\n",
    "        })\n",
    "\n",
    "# Save to file\n",
    "export_file = 'graph_analysis_export.json'\n",
    "with open(export_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Graph data exported to {export_file}\")\n",
    "print(f\"\\n📊 Export Summary:\")\n",
    "print(f\"  - Documents: {len(export_data['documents'])}\")\n",
    "print(f\"  - Sample chunks: {len(export_data['sample_chunks'])}\")\n",
    "print(f\"  - Total size: {len(json.dumps(export_data))} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "rag.close()\n",
    "print(\"✅ Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive analysis of your Neo4j knowledge graph:\n",
    "\n",
    "- **Graph Statistics**: Document and chunk counts, size distributions\n",
    "- **Category Analysis**: Distribution of content across categories\n",
    "- **Connectivity**: Relationship integrity and orphaned node detection\n",
    "- **Embedding Coverage**: Verification of vector embeddings\n",
    "- **Search Patterns**: Similarity score distributions\n",
    "- **Knowledge Coverage**: Topic-category heatmap\n",
    "- **Data Export**: JSON export for external analysis\n",
    "\n",
    "### Next Steps:\n",
    "- Explore **04_knowledge_discovery.ipynb** for semantic exploration\n",
    "- Use **05_query_optimization.ipynb** for performance tuning\n",
    "- Review exported data for insights about your knowledge base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}